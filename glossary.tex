\makeglossaries

\newglossaryentry{overfitting}
{
    name=overfitting,
    description={Succede quando il sistema tende a identificare relazioni nell’insieme di addestramento che non valgono in generale cioè che il sistema impara a memoria, questo porta a ottimi risultati nel data set e pessimi risultati nel test set o negli usi reali}
}
\newglossaryentry{fully-connected}
{
    name=fully-connected,
    description={I livelli completamente connessi in una rete neurale sono quei livelli in cui tutti gli input di
    un livello sono collegati a ogni unità di attivazione del livello successivo. Normalmente nei modelli di
    apprendimento automatico più diffusi, gli ultimi livelli sono livelli completamente connessi
    che compilano i dati estratti dai livelli precedenti per formare l’output finale. È il secondo
    livello più dispendioso in termini di tempo, dopo il Convolution Layer}
}
\longnewglossaryentry{convoluzione}
{
    name=convoluzione}{Essa  è una operazione tipica dell'elaborazione dei segnali e più in generale delle immagini, che nel caso di un una immagine discreta è:\[ \mathbf { I } * \mathbf{F} [ i , j ] = \sum _ { y = 1 } ^ { m } \sum _ { x = 1 } ^ { m } \left( \mathbf { I } \left[ i + \left\lceil \frac { m } { 2 } \right\rceil - y , j + \left\lceil \frac { m } { 2 } \right] - x \right] \cdot \mathbf { F } [ y , x ] \right) \]
    Dove \(\mathbf{F}\) è il filtro definito sulla griglia \(m \times m\), \(\mathbf{I}\) è la nostra matrice immagine, e \(\mathbf { I } * \mathbf{F}\) sono i nostri pixel filtrati.

    La Complessità
    computazionale  della convoluzione è 
    piuttosto elevata: data 
    un’immagine di \(n\times n\)
    pixel e un filtro di
    \(m\times m\)
    elementi, la 
    convoluzione 
    richiede \(m^2 n^2\)
    moltiplicazioni e
    altrettante somme. 
    
    La soluzione è lavorare nello spazio di Fourier, ricordando che la FFT (Fast Fourier Transform) si ottiene con un procedimento \(O(n \log n)\), ricordando che dati \( f \) e \( g \) due funzioni la cui convoluzione è indicata da \( f * g \). Sia \( \mathcal { F } \) roperatore trasformata di Fourier,  \( \mathcal { F } \{ f \} \) e \( \mathcal { F } \{ g \} \) sono le trasformate di \( f \) e \( g \) rispettivamente. Allora:
    \( \mathcal { F } \{ f * g \} = \mathcal { F } \{ f \} \cdot \mathcal { F } \{ g \} \)}

    
\longnewglossaryentry{stride} {
    name=stride} {Lo stride regola la dimensione tra pixel di ingresso e pixel in uscita, in particolare quando un filtro viene fatto scorrere sul volume di input, invece di spostarsi con passiunitari (di 1 neurone) si può utilizzare un passo (o stride) maggiore. Questa operazioneriduce la dimensione delle feature map nel volume di output e conseguentemente il numerodi connessioni. Lo stride è spesso impostato su un numero intero, anziché su una frazione oun decimale.
    
    Sui livelli iniziali della rete per piccoli stride (es. 2, 4), è possibile ottenere un elevatoguadagno in eﬀicienza a discapito di una leggera penalizzazione in accuratezza.}

\newglossaryentry{padding} {
    name=padding,
    description= {Per evitare che i pixel laterali abbiano una minore influenza sull'output di una convoluzione è tipico aggiungere un zero-padding (un bordo di pixel con valore zero) per poter filtrare anche i pixel laterali dell’immagine, che altrimenti verrebbero analizzati da un numero inferiore di filtri in quanto i filtri non possono uscire dalla matrice, e portando anche un portando quindi ad un shrinking (riduzione di dimensione) dell’outpute a una perdita di informazioni }
}
\newglossaryentry{Convoluion Layer}
{
    name={Convoluion Layer},
    description={È un layer che prende in input una immagine e ne torna fuori un’altra immagine dopo aver effettuato una convoluzione.}
}
\newglossaryentry{ReLu}
{
    name=ReLu,
    description={La funzione di attivazione ReLu (Rectified Linear) è nata per risolvere alcuni problemi delle funzioni di attivazione usate nelle reti MLP, come la sigmoide, in quanto essa da problemi nella retroprograzione del gradiente. La funzione è definita come \(f(x) = \max (0, x)\)}
}

\longnewglossaryentry{LearningRate}{name={learning rate}}{La learning rate, o velocità di apprendimento è un parametro di ottimizzazione in un algoritmo di ottimizzazione, normalmente il gradient descent optimization algorithm. Il learning rate è un iperparametro che determina quanto modificare i pesi in base all'attuale errore. La scelta del tasso di apprendimento è impegnativa in quanto un valore troppo piccolo può comportare un lungo processo di allenamento che potrebbe bloccarsi, mentre un valore troppo grande può comportare l'apprendimento di una serie di pesi non ottimale troppo velocemente o un processo di allenamento instabile. 

La velocità di apprendimento può essere l'iperparametro più importante durante la configurazione della rete neurale. Pertanto è fondamentale sapere come indagare gli effetti del tasso di apprendimento sulle prestazioni del modello e costruire un'intuizione sulla dinamica del tasso di apprendimento sul comportamento del modello.}

\longnewglossaryentry{MiniBatchSize}{name={Mini-batch Size}}{
    La dimensione del batch definisce il numero di campioni che verranno analizzati attraverso la rete in un turno. 
    
    Ad esempio, supponiamo che tu abbia 1050 campioni di addestramento e desideri impostare un valore batch size uguale a 100. L'algoritmo prende i primi 100 campioni (dal 1° al 100° dal set di dati di addestramento e addestra la rete. Successivamente, prende i secondi 100 campioni (dal 101° al 200°) e addestra nuovamente la rete. Possiamo continuare a eseguire questa procedura finché non abbiamo propagato tutti i campioni attraverso la rete. Il problema potrebbe verificarsi con l'ultima serie di campioni. Nel nostro esempio, abbiamo usato 1050 che non è divisibile per 100 senza resto. La soluzione più semplice è ottenere gli ultimi 50 campioni e addestrare la rete. Si parla di Mini-batch quando la dimensione del batch è molto piccola. 
    
    I vantaggi dell'uso di una batch size minore al numero totale dei campioni:
\begin{itemize}
    \item Poiché si addestra la rete utilizzando meno campioni, la procedura di addestramento complessiva richiede meno memoria. Ciò è particolarmente importante se non sei in grado di inserire l'intero set di dati nella memoria della tua macchina.
    \item In genere le reti si addestrano più velocemente con i mini-batch. Questo perché aggiorniamo i pesi dopo ogni propagazione. Nel nostro esempio abbiamo propagato 11 batch (10 di loro avevano 100 campioni e 1 aveva 50 campioni) e dopo ciascuno di essi abbiamo aggiornato i parametri della nostra rete. Se usassimo tutti i campioni durante la propagazione, faremmo solo 1 aggiornamento per il parametro di rete.
\end{itemize}
È da sottolineare che più piccolo è il batch, meno accurata sarà la stima del gradiente.}