\makeglossaries

\newglossaryentry{overfitting}
{
    name=overfitting,
    description={Succede quando il sistema tende  a identificare relazioni nell’insieme di addestramento che non valgono in generale cioè che il sistema impara a memo-ria}
}
\newglossaryentry{fully-connected}
{
    name=fully-connected,
    description={}
}
\newglossaryentry{convoluzione}
{
    name=convoluzione,
    description={I livelli completamente connessi in una rete neurale sono quei livelli in cui tutti gli input di
    un livello sono collegati a ogni unità di attivazione del livello successivo. Normalmente nei modelli di
    apprendimento automatico più diffusi, gli ultimi livelli sono livelli completamente connessi
    che compilano i dati estratti dai livelli precedenti per formare l’output finale. È il secondo
    livello più dispendioso in termini di tempo, dopo il Convolution Layer.}
}
\newglossaryentry{ReLu}
{
    name=ReLu,
    description={La funzione di attivazione ReLu (Rectified Linear) è nata per risolvere alcuni problemi delle funzioni di attivazione usate nelle reti MLP, come la sigmoide, in quanto essa da problemi nella retroprograzione del gradiente. La funzione è definita come \(f(x) = \max (0, x)\)}
}